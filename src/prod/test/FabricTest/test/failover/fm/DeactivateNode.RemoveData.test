#
# [owner] MMohsin
#
# This test case verifies the replica placement when a node is deactivated with intent RemoveData.
#

votes 10 20 30

# --> FM, CM and Naming are constrainted to nodes 10, 20, 30 and 40.
# --> Their target replica set size is 3.
# The above values have been carefully chosen based on the requirements
# of this test. Please consider the following points before changing these
# values.
#  (1) In Scenario 14, we take down nodes 90, 80 and 70. By constraining
#      FM, CM and Naming to nodes 10, 20, 30 and 40 we ensure that they do not
#      lose any replicas when the nodes are taken down. This is how we ensure
#      that FM, CM and Naming don't go into quorum loss in Scenario 14.
#
#  (2) In Scenario 5, node 10 gets deactivated with intent remove-data.
#      For deactivation with intent remove-data, the replicas on the node
#      will have to be replaced with new replicas on other nodes. Since FM,
#      CM and Naming replicas are constrainted to nodes 10, 20, 30 and 40, the
#      deactivation of node 10 with intent remove-data leaves only nodes 20,
#      30 and 40 to host these services. This is why we set the target replica
#      set size of these services to 3, so that nodes 20, 30 and 40 are enough
#      to host them. Otherwise, deactivation could get blocked due to FM, CM
#      or Naming.
set FMPlacementConstraints system==true
set CMPlacementConstraints system==true
set NamingPlacementConstraints system==true
fmservice 3 2
cmservice 3 2
namingservice 1 3 2
cleantest

set DummyPLBEnabled true
set ExpectedClusterSize 3
set UserReplicaRestartWaitDuration 99999
set NamingOperationTimeout 10

+10 nodeprops=system:true
+20 nodeprops=system:true
+30 nodeprops=system:true
verify

+40 nodeprops=system:true
+50 nodeprops=system:false
verify

################################################################################
# Scenario 1: Base case:
#             1. Deactivate a node with intent RemoveData
#             2. Ensure all the replicas are moved out of that node
#             3. Ensure no new replicas are created on that node
#             4. Ensure only expected number of new replicas are created
################################################################################

createservice fabric:/stateless1 CalculatorServiceType n 1 3
createservice fabric:/volatile1 TestStoreServiceType y 1 2
createservice fabric:/volatile2 TestStoreServiceType y 1 3 minreplicasetsize=2
createservice fabric:/volatile3 TestStoreServiceType y 1 3 minreplicasetsize=3
createservice fabric:/persisted1 TestPersistedStoreServiceType y 1 2 persist
createservice fabric:/persisted2 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=2
createservice fabric:/persisted3 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3
verify

# Verify the initial replica count on each node
!waitforstate FM.UpReplicaCount.10 2
!waitforstate FM.UpReplicaCount.20 2
!waitforstate FM.UpReplicaCount.30 7
!waitforstate FM.UpReplicaCount.40 7
!waitforstate FM.UpReplicaCount.50 7

# Deactivate node 50 with intent RemoveData
DeactivateNode 50 RemoveData
!waitforstate FM.Node.DeactivationIntent.50 RemoveData
!waitforstate FM.Node.DeactivationStatus.50 DeactivationComplete

# Create some more services
createservice fabric:/stateless2 CalculatorServiceType n 1 4
createservice fabric:/volatile4 TestStoreServiceType y 1 4 minreplicasetsize=3
createservice fabric:/persisted4 TestPersistedStoreServiceType y 1 4 persist minreplicasetsize=3
!wait

# Verify that the up replica count for node 50 is zero
!waitforstate FM.UpReplicaCount.10 5
!waitforstate FM.UpReplicaCount.20 10
!waitforstate FM.UpReplicaCount.30 12
!waitforstate FM.UpReplicaCount.40 10
!waitforstate FM.UpReplicaCount.50 0

# The persisted service replicas on node 50 should have been moved.
# Verify that the offline replica count for node 50 is zero.
!waitforstate FM.OfflineReplicaCount.50 0

ActivateNode 50
!waitforstate FM.Node.DeactivationIntent.50 None
!waitforstate FM.Node.DeactivationStatus.50 None
verify

################################################################################
# Scenario 2: Special case when TargetReplicaSetSize = 1
################################################################################

# Create some more services
createservice fabric:/volatile5 TestStoreServiceType y 1 1
createservice fabric:/persisted5 TestPersistedStoreServiceType y 1 1 persist
verify

!waitforstate FM.Replica.Role.fabric:/volatile5.50 Primary
!waitforstate FM.Replica.Role.fabric:/persisted5.50 Primary

DeactivateNode 50 RemoveData
!waitforstate FM.Node.DeactivationIntent.50 RemoveData
!waitforstate FM.Node.DeactivationStatus.50 DeactivationComplete
verify

ActivateNode 50
!waitforstate FM.Node.DeactivationIntent.50 None
!waitforstate FM.Node.DeactivationStatus.50 None
verify

################################################################################
# Scenario 3: Ensure primary is swapped out first before creating a replacement
#             replica
################################################################################

!waitforstate FM.Replica.Role.fabric:/volatile1.40 Primary
!waitforstate FM.Replica.Role.fabric:/volatile2.40 Primary
!waitforstate FM.Replica.Role.fabric:/volatile3.40 Primary
!waitforstate FM.Replica.Role.fabric:/volatile4.40 Primary
!waitforstate FM.Replica.Role.fabric:/volatile5.40 Primary
!waitforstate FM.Replica.Role.fabric:/persisted1.40 Primary
!waitforstate FM.Replica.Role.fabric:/persisted2.40 Primary
!waitforstate FM.Replica.Role.fabric:/persisted3.40 Primary
!waitforstate FM.Replica.Role.fabric:/persisted4.40 Primary
!waitforstate FM.Replica.Role.fabric:/persisted5.40 Primary

addbehavior b1 * * AddReplica

DeactivateNode 40 RemoveData

!waitforstate FM.Replica.Role.fabric:/volatile1.30 Primary
!waitforstate FM.Replica.Role.fabric:/volatile2.30 Primary
!waitforstate FM.Replica.Role.fabric:/volatile3.30 Primary
!waitforstate FM.Replica.Role.fabric:/volatile4.30 Primary
!waitforstate FM.Replica.Role.fabric:/volatile5.50 Idle
!waitforstate FM.Replica.Role.fabric:/persisted1.30 Primary
!waitforstate FM.Replica.Role.fabric:/persisted2.30 Primary
!waitforstate FM.Replica.Role.fabric:/persisted3.30 Primary
!waitforstate FM.Replica.Role.fabric:/persisted4.30 Primary
!waitforstate FM.Replica.Role.fabric:/persisted5.50 Idle

removebehavior b1

!waitforstate FM.Node.DeactivationIntent.40 RemoveData
!waitforstate FM.Node.DeactivationStatus.40 DeactivationComplete
verify

ActivateNode 40
!waitforstate FM.Node.DeactivationIntent.40 None
!waitforstate FM.Node.DeactivationStatus.40 None
verify

################################################################################
# Scenario 4: Safety check should not consider partitions that have no data
################################################################################

+60 nodeprops=system:false
verify

addbehavior b1 * * AddPrimaryReply
createservice fabric:/persisted6 TestPersistedStoreServiceType y 1 3 persist minreplicasetsize=3

!waitforstate FM.Replica.Role.fabric:/persisted6.60 Primary
!waitforstate RA.Replica.State.fabric:/persisted6.60.60 RD

removeruntime 60 y
!waitforstate FM.Replica.IsUp.fabric:/persisted6.60 false

removebehavior b1

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationComplete

ActivateNode 60
!waitforstate FM.Node.DeactivationIntent.60 None
!waitforstate FM.Node.DeactivationStatus.60 None

addruntime 60 y
!waitforstate FM.Replica.IsUp.fabric:/persisted6.60 true

verify

################################################################################
# Scenario 5: Safety check should not consider partitions that are getting
#             deleted
################################################################################

!waitforstate FM.Replica.Role.fabric:/persisted6.60 Primary
!waitforstate FM.Replica.Role.fabric:/persisted6.50 Secondary
!waitforstate FM.Replica.Role.fabric:/persisted6.40 Secondary

movesecondary fabric:/persisted6 40 10
!waitforstate FM.Replica.Role.fabric:/persisted6.10 Secondary
verify

removeruntime 10 y
!waitforstate FM.Replica.IsUp.fabric:/persisted6.10 false

removeruntime 60 y
!waitforstate FM.Replica.IsUp.fabric:/persisted6.60 false

deleteservice fabric:/persisted6 errors=Timeout

DeactivateNodes batch5 10:RemoveData,60:RemoveData
VerifyNodeDeactivationStatus batch5 DeactivationComplete

addruntime 10 y
addruntime 60 y

RemoveNodeDeactivation batch5

deleteservice fabric:/persisted6 errors=UserServiceNotFound
verify

################################################################################
# Scenario 6: ToBeDropped flag for node deactivation should get cleared if
#             the node gets activated
################################################################################

createservice fabric:/foo TestStoreServiceType y 1 6
verify

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 5

!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!waitforstate FM.Replica.IsToBeDropped.fabric:/foo.60 true

ActivateNode 60
!waitforstate FM.Node.DeactivationIntent.60 None
!waitforstate FM.Node.DeactivationStatus.60 None

!waitforstate FM.Replica.IsToBeDropped.fabric:/foo.60 false

deleteservice fabric:/foo
verify

################################################################################
# Scenario 7: Node deactivation should not automatically complete if the node
#             is down
################################################################################

createservice fabric:/persisted7 TestPersistedStoreServiceType y 1 1 persist
verify

-60
!waitforstate FM.Replica.IsUp.fabric:/persisted7.60 false

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 5

!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

+60 nodeprops=system:false
verify

ActivateNode 60
!waitforstate FM.Node.DeactivationIntent.60 None
!waitforstate FM.Node.DeactivationStatus.60 None

deleteservice fabric:/persisted7
verify

################################################################################
# Scenario 8: Remove Data should send a message to the node 
# Node should use it to stop opening replicas and send back explicit reply
################################################################################

createservice fabric:/persisted8 TestPersistedStoreServiceType y 3 3 persist
verify

removeruntime 60 y
!waitforstate FM.Replica.IsUp.fabric:/persisted8.60 false

addbehavior b0 * * NodeDeactivateRequest

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckComplete

injectfailure 60 fabric:/persisted8 service.beginopen

addruntime 60 y
!waitforstate RA.Replica.IsUp.fabric:/persisted8.60.60 true

removebehavior b0
!waitforstate RA.Replica.IsUp.fabric:/persisted8.60.60 false

!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationComplete

removefailure 60 fabric:/persisted8 service.beginopen
ActivateNode 60
verify

deleteservice fabric:/persisted8
verify

################################################################################
# Scenario 9: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario min is 5 so it is fine to proceed
################################################################################

set RemoveNodeOrDataUpReplicaTimeout 20

createservice fabric:/persisted9 TestPersistedStoreServiceType y 6 6 persist minreplicasetsize=5
verify

!waitforstate FM.Replica.IsUp.fabric:/persisted9.60 true

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData

!pause 10
#timeout has not expired
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 10
!waitforstate FM.Replica.Role.fabric:/persisted9.60 Idle
!waitforstate FM.Node.DeactivationStatus.60 DeactivationComplete

ActivateNode 60

deleteservice fabric:/persisted9
verify

################################################################################
# Scenario 10: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario min is 5 and one replica is down so it is NOT fine to proceed
################################################################################

set RemoveNodeOrDataUpReplicaTimeout 5

createservice fabric:/persisted10 TestPersistedStoreServiceType y 6 6 persist minreplicasetsize=5
verify

!waitforstate FM.Replica.IsUp.fabric:/persisted10.60 true
!waitforstate FM.Replica.IsUp.fabric:/persisted10.30 true
-30

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 10

!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress


ActivateNode 60

+30 nodeprops=system:true

deleteservice fabric:/persisted10
verify

################################################################################
# Scenario 11: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario this setting is big so we cannot proceed in this timeframe
################################################################################

set RemoveNodeOrDataUpReplicaTimeout 1000

createservice fabric:/persisted11 TestPersistedStoreServiceType y 6 6 persist minreplicasetsize=3
verify

!waitforstate FM.Replica.IsUp.fabric:/persisted11.60 true

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 10

!waitforstate FM.Node.DeactivationIntent.60 RemoveData
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

ActivateNode 60

deleteservice fabric:/persisted11
verify

################################################################################
# Scenario 12: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario reconfiguration is stuck so we cannot proceed even though the timeout has expired
################################################################################

set RemoveNodeOrDataUpReplicaTimeout 5

createservice fabric:/persisted12 TestPersistedStoreServiceType y 6 6 persist minreplicasetsize=5
verify


addbehavior b1 * * DoReconfiguration

!waitforstate FM.Replica.IsUp.fabric:/persisted12.60 true

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData

!pause 5
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

!pause 10
!waitforstate FM.Node.DeactivationStatus.60 DeactivationSafetyCheckInProgress

removebehavior b1

ActivateNode 60

deleteservice fabric:/persisted12
verify

################################################################################
# Scenario 13: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario this config value is 0 so we should move to deactivation complete immediately
################################################################################

set RemoveNodeOrDataUpReplicaTimeout 0

createservice fabric:/persisted13 TestPersistedStoreServiceType y 6 6 persist minreplicasetsize=5
verify

!waitforstate FM.Replica.IsUp.fabric:/persisted13.60 true

DeactivateNode 60 RemoveData
!waitforstate FM.Node.DeactivationIntent.60 RemoveData

!waitforstate FM.Node.DeactivationStatus.60 DeactivationComplete

ActivateNode 60

deleteservice fabric:/persisted13
verify


################################################################################
# Scenario 14: Remove Data should wait for an up replica to disappear from the node
# In case RemoveNodeOrDataUpReplicaTimeout expires it is fine to proceed further in case no other safety checks prevent it
# In this scenario the partition is already in quorum loss. We are testing that we protect against data loss by ensuring
# that we do not drop a write quorum of replicas from the current configuration.
################################################################################

deleteservice fabric:/persisted1
deleteservice fabric:/persisted2
deleteservice fabric:/persisted3
deleteservice fabric:/persisted4
deleteservice fabric:/persisted5

set RemoveNodeOrDataUpReplicaTimeout 15

+70 nodeprops=system:false
+80 nodeprops=system:false
+90 nodeprops=system:false
+100 nodeprops=system:false
verify

createservice fabric:/persisted14 TestPersistedStoreServiceType y 1 6 persist minreplicasetsize=6 constraint=(system==false)
verify

-90
-80
-70
!waitforstate FM.Replica.IsUp.fabric:/persisted14.90 false
!waitforstate FM.Replica.IsUp.fabric:/persisted14.80 false
!waitforstate FM.Replica.IsUp.fabric:/persisted14.70 false
!waitforstate FM.FT.QuorumLost.fabric:/persisted14 true

# Drop one replica directly by reporting fault permanent
reportfault 100 fabric:/persisted14 permanent
!waitforstate FM.Replica.State.fabric:/persisted14.100 Dropped

# Drop second replica by deactivating the node with intent RemoveData
DeactivateNode 90 RemoveData
!waitforstate FM.Node.DeactivationIntent.90 RemoveData
!waitforstate FM.Node.DeactivationStatus.90 DeactivationComplete

# Try to drop third replica by deactivating the node with intent RemoveData.
# The deactivation should block infinitely due to safety check because we want
# to avoid dropping a write quorum of replicas.
DeactivateNode 80 RemoveData
# Wait for longer than RemoveNodeOrDataUpReplicaTimeout to make sure safety
# check still blocks.
!pause 30
!waitforstate FM.Node.DeactivationIntent.80 RemoveData
!waitforstate FM.Node.DeactivationStatus.80 DeactivationSafetyCheckInProgress

ActivateNode 90
ActivateNode 80

+70 nodeprops=system:false
+80 nodeprops=system:false
+90 nodeprops=system:false
verify

deleteservice fabric:/persisted14
verify

!q
