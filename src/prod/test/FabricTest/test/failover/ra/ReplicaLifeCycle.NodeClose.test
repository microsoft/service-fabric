#
# [owner] aprameyr
# Test scenarios around node going down and replicas being closed
set ServiceTypeDisableGraceInterval 100000
set NamingOperationTimeout 10
set GracefulReplicaShutdownMaxDuration 60
set AsyncNodeCloseEnabled true
set StartNodeCloseOnSeparateThread true
set FMPlacementConstraints system==true

votes 10 20 30
cmservice 1 1
fmservice 3 1
namingservice 1 1 1
cleantest

+10 nodeprops=system:true
+20 nodeprops=system:true
+30 nodeprops=system:true
verify

app.add version10 TestApp 1.0
app.clear version10
app.servicepack version10 ServicePackageA version=1.0
app.servicetypes version10 ServicePackageA ServiceTypeA1 stateful persist
app.codepack version10 ServicePackageA CodeA1 types=ServiceTypeA1 version=1.0
app.upload version10
provisionapp version10

createapp fabric:/app TestApp 1.0
verify

+40 nodeprops=system:false
verify

#####################################################
# Scenario 1: Replica down should be sent to FM 
#####################################################
createservice fabric:/app/test1a TestApp_App0:ServicePackageA:ServiceTypeA1 y 1 1 persist constraint=(system!=true) appname=fabric:/app
createservice fabric:/app/test1b TestApp_App0:ServicePackageA:ServiceTypeA1 y 1 1 persist constraint=(system!=true) appname=fabric:/app
verify

!var PartitionIdService1a FM.FT.PartitionId.fabric:/app/test1a

client.addbehavior b0 40 * ReplicaClose 1.0 Max 0.0 0 -1 filters=PartitionId:<var.PartitionIdService1a>
!pause 5

# take down the node. 
# the logic here is relying on an implementation detail
# test1b is in a fabric test factory so it will be shutdown prior to the 
-40
!waitforstate FM.Replica.IsUp.fabric:/app/test1b.40 false
!waitforstate FM.Replica.IsUp.fabric:/app/test1a.40 true
!waitforstate FM.Node.IsUp.40 true

# the node close should still be blocked at RA
!pause 30
!waitforstate FM.Node.IsUp.40 true

# now remove the block
client.removebehavior * 40
!waitforstate FM.Node.IsUp.40 false

+40 nodeprops=system:false
verify

deleteservice fabric:/app/test1a 
deleteservice fabric:/app/test1b 
verify

#####################################################
# Scenario 2: Node close should not be blocked for past the duration
#####################################################
createservice fabric:/app/test2 TestApp_App0:ServicePackageA:ServiceTypeA1 y 1 1 persist constraint=(system!=true) appname=fabric:/app
verify

client.addbehavior b0 40 * ReplicaClose 1.0 Max 0.0 0 -1 filters=PartitionId:<var.PartitionIdService1a>
!pause 5

-40
!waitforstate FM.Node.IsUp.40 true

# the node close should still be blocked at RA
!pause 90
!waitforstate FM.Node.IsUp.40 false

+40 nodeprops=system:false
verify

client.removebehavior * 40
deleteservice fabric:/app/test2 
verify

#####################################################
# Scenario 3: If FM moves while node is closing, replica down should be sent to new FM
#####################################################

# Later in the test, we will block the close message to prevent the replica and hence
# the node from closing. In order to make this work, we also need to set
# GracefulReplicaShutdownMaxDuration to a high value. RA will forcibly close the replica
# and the node after GracefulReplicaShutdownMaxDuration elapses. So if this value is too
# low, it will defeat the mechanism that we are using in this test to block the close.
set GracefulReplicaShutdownMaxDuration 3600

createservice fabric:/app/test3a TestApp_App0:ServicePackageA:ServiceTypeA1 y 1 1 persist constraint=(system!=true) appname=fabric:/app
createservice fabric:/app/test3b TestApp_App0:ServicePackageA:ServiceTypeA1 y 1 1 persist constraint=(system!=true) appname=fabric:/app
verify

!var PartitionIdService3a FM.FT.PartitionId.fabric:/app/test3a
!var PartitionIdService3b FM.FT.PartitionId.fabric:/app/test3b

addbehavior bBlockService3aClose 40 * ReplicaClose 1.0 Max 0.0 0 -1 PartitionId:<var.PartitionIdService3a>
addbehavior bBlockService3bClose 40 * ReplicaClose 1.0 Max 0.0 0 -1 PartitionId:<var.PartitionIdService3b>

# take down the node and pause for some time
-40
!pause 18

# the node close should still be blocked at RA because the close is blocked.
!waitforstate FM.Replica.IsUp.fabric:/app/test3a.40 true
!waitforstate FM.Replica.IsUp.fabric:/app/test3b.40 true
!waitforstate FM.Node.IsUp.40 true

# move the FM primary and pause for some time
!var FMServiceFTID FM.FT.PartitionId.FMService
moveprimaryclient RANDOM <var.FMServiceFTID>
!pause 3

# now remove the block for one of the services
removebehavior bBlockService3aClose

# The new FM primary should get notifed about the closing of replica for service 3a
!waitforstate FM.Replica.IsUp.fabric:/app/test3a.40 false

# pause for some more time
!pause 5

# now remove the block for the other service also
removebehavior bBlockService3bClose

# wait for replica of service 3b to close and for the node to close
!waitforstate FM.Replica.IsUp.fabric:/app/test3b.40 false
!waitforstate FM.Node.IsUp.40 false

# Bring the node back up and wait for the services to stabilize
+40 nodeprops=system:false
!pause 25
verify

deleteservice fabric:/app/test3a
deleteservice fabric:/app/test3b
verify

!q
